\chapter[Học khái niệm trong logic mô tả sử dụng mô phỏng hai~chiều]{HỌC KHÁI NIỆM TRONG LOGIC MÔ TẢ\\SỬ DỤNG MÔ PHỎNG HAI CHIỀU} 
\label{Chapter3}
\thispagestyle{fancy}

%-------------------------------------------------------------------
\section{Giới thiệu}
\label{sec:Chap3.Introduction}
Trong những năm gần đây, logic mô tả đã và đang được ứng dụng trong nhiều lĩnh vực của Web ngữ nghĩa. Nó được xem là một thành phần quan trọng trong các hệ thống ngữ nghĩa và được sử dụng để biểu diễn cũng như suy luận tri thức trong các hệ thống này. Đặc tả các khái niệm phù hợp cho các hệ thống ngữ nghĩa là một trong những vấn đề rất được quan tâm. Do vậy, vấn đề đặt ra là cần tìm được các khái niệm quan trọng và xây dựng được định nghĩa của các khái niệm đó. Học khái niệm trong logic mô tả nhằm mục đích kiểm tra, suy luận và tìm ra được các khái niệm này để phục vụ cho các ứng dụng khác nhau như: tin sinh học, tin học trong y tế, quản trị tri thức, kỹ nghệ phần mềm,\,\ldots

Vấn đề học khái niệm cho cơ sở tri thức trong logic mô tả của đề tài này được đặt ra theo ngữ cảnh sau:

\noindent
\ramka{
Cho cơ sở tri thức $\KB$ trong logic mô tả $L$ và các tập các cá thể $E^+$, $E^-$. Học khái niệm $C$ trong $L$ sao cho:
	\begin{enumerate}
		\item $\KB \models C(a)$ với mọi $a \in E^+$, và
		\item $\KB \not\models C(a)$ với mọi $a \in E^-$,
	\end{enumerate}
	trong đó, tập $E^+$ chứa các mẫu dương và $E^-$ chứa các mẫu âm của $C$.
}

%-------------------------------------------------------------------
\section{Phân hoạch miền của diễn dịch trong logic mô tả}

Cho diễn dịch $\mI = \tuple{\Delta^\mI, \cdot^\mI}$ trong ngôn ngữ $\mLSP$. Ý tưởng cơ bản của việc làm mịn phân hoạch miền $\Delta^\mI$ của diễn dịch $\mI$ là dựa trên phương pháp học khái niệm sử dụng mô phỏng hai chiều. Bắt đầu từ phân hoạch $\{\Delta^\mI\}$, chúng ta thực hiện làm mịn liên tục $\{\Delta^\mI\}$ để đạt được phân hoạch tương ứng với quan hệ $\simSdPdI$ với các kỹ thuật cụ thể đặt ra như sau~\cite{Nguyen2013}:
\begin{itemize}
	\item Quá trình làm mịn có thể dừng lại khi một số điều kiện đặt ra được thỏa mãn.		
	\item Trong quá trình làm mịn phân hoạch $\{\Delta^\mI\}$, các khối được tạo ra ở tất cả các bước được ký hiệu là $Y_1, Y_2, \ldots, Y_n$. Để thực hiện được điều này, khi mỗi khối được tạo ra, chúng ta sử dụng một chỉ số mới gán cho khối đó bằng cách tăng giá trị của~$n$. Ta gọi phân hoạch hiện thời là $\mbY = \{Y_{i_1}, Y_{i_2}, \ldots, Y_{i_k}\} \subseteq \{Y_1, Y_2, \ldots, Y_n\}$. Như vậy, $\mbY$ là một tập con của các khối được tạo ra ở trên. Chúng ta thiết lập thông tin nhằm ghi nhận lại khái niệm $C_i$ đặc trưng cho khối $Y_i$ sao cho $C_i^\mI = Y_i$.
	\item Chúng ta có thể sử dụng các chiến lược khác nhau cũng như các hàm tính điểm để tối ưu hóa quá trình làm mịn.
\end{itemize}

\subsection{Bộ chọn cơ bản}
\label{sec:Chap3.BasicSelectors}

\begin{Definition}[Bộ chọn cơ bản]
	\label{def:BasicSelectors}
	Một {\em bộ chọn cơ bản} trong $\mLSPD$ dùng để phân chia khối~$Y_{i_j}$ của phân hoạch $\mbY = \{Y_{i_1}, Y_{i_2}, \ldots, Y_{i_k}\}$ là một khái niệm thuộc một trong các dạng~sau:
	\begin{itemize}
		\item $A$, trong đó $A \in \SigmaDagC$,
%		
		\item $A=d$, trong đó $A \in \SigmaDagA\setminus\SigmaDagC$ và $d \in \Range(A)$,
%		
		\item $\E \sigma.\{d\}$, trong đó $\sigma \in \SigmaDagDR$ và $d \in \Range(\sigma)$,
%
		\item $\E r.C_{i_t}$, trong đó $r \in \SigmaDagOR$ và $1 \leq t \leq k$,
%		
		\item $\E r^-.C_{i_t}$, nếu $\mI \in \Phi^\dag$, $r \in \SigmaDagOR$ và $1 \leq t \leq k$,
%
		\item $\{a\}$, nếu $\mO \in \Phi^\dag$ và $a \in \SigmaDagI$,
%		
		\item $\leq\!1\,r$, nếu $\mF \in \Phi^\dag$ và $r \in \SigmaDagOR$,
%		
		\item $\leq\!1\,r^-$, nếu $\{\mF, \mI\} \subseteq \Phi^\dag$ và $r \in \SigmaDagOR$,
%
		\item $\geq\!l\,r$ và $\leq\!m\,r$, nếu $\mN \in \Phi^\dag$, $r \in \SigmaDagOR$, $0 < l \leq \#\Delta^\mI$ và $0 \leq m < \#\Delta^\mI$,
%
		\item $\geq\!l\,r^-$ và $\leq\!m\,r^-$, nếu $\{\mN, \mI\}\!\subseteq \Phi^\dag$, $r\!\in\!\SigmaDagOR$, $0 <\!l \leq\! \#\Delta^\mI$ và $0 \leq m < \#\Delta^\mI$,
%
		\item $\geq\!l\,r.C_{i_t}$ và $\leq\!m\,r.C_{i_t}$, nếu $\mQ \in \Phi^\dag$, $r \in \SigmaDagOR$, $1 \leq t \leq k$, $0 < l \leq \#C_{i_t}$ và $0 \leq m < \#C_{i_t}$,
%
		\item $\geq\!l\,r^-.C_{i_t}$ và $\leq\!m\,r^-.C_{i_t}$, nếu $\{\mQ, \mI\} \subseteq \Phi^\dag$, $r \in \SigmaDagOR$, $1 \leq t \leq k$, $0 < l \leq \#C_{i_t}$ và $0 \leq m < \#C_{i_t}$,
%
		\item $\E r.\Self$, nếu $\Self \in \Phi^\dag$ và $r \in \SigmaDagOR$.\myend
	\end{itemize}
\end{Definition}

\begin{Theorem}
\label{th:BasicSelectors}
	Cho $\Sigma$ và $\SigmaDag$ là các bộ ký tự logic mô tả sao cho $\SigmaDag \subseteq \Sigma$, $\Phi$ và $\PhiDag$ là tập các đặc trưng logic mô tả sao cho $\PhiDag \subseteq \Phi$, $\mI$ là một diễn dịch hữu hạn trong $\mLSP$.
	Xuất phát từ phân hoạch $\{\Delta^\mI\}$ và thực hiện việc làm mịn liên tục nó bằng các bộ chọn cơ bản ta sẽ nhận được một phân hoạch tương ứng với quan hệ tương đương~$\sim_\SdPdI$.\myend
\end{Theorem}

Trong thực tế, để quá trình phân hoạch đạt hiệu quả cao, chúng ta có thể xem xét sử dụng các bộ chọn được trình bày trong Hình~\ref{fig:OtherSelectors}.
\begin{figure}
\ramka{
	\vspace{-2.0ex}
	\begin{itemize}
		\item $A < d$, $A \leq d$, trong đó $A \in \SigmaDagNA$, $d \in \Range(A)$ và $d$ không phải là giá trị nhỏ nhất của~$\Range(A)$,
	%
		\item $A > d$, $A \geq d$, trong đó $A \in \SigmaDagNA$, $d \in \Range(A)$ và $d$ không phải là giá trị lớn nhất của~$\Range(A)$,
	%
		\item $\E r.\top$, $\E r.C_i$ và $\V r.C_i$, trong đó $r \in \SigmaDagOR$ và $1 \leq i \leq n$,
	%		
		\item $\E r^-.\top$, $\E r^-.C_i$, $\V r^-.C_i$, nếu $\mI \in \Phi^\dag$, $r \in \SigmaDagOR$ và $1 \leq i \leq n$,
	%
		\item $\geq\!l\,r.C_i$ và $\leq\!m\,r.C_i$, nếu $\mQ \in \Phi^\dag$, $r \in \SigmaDagOR$, $1 \leq i \leq n$, $0 < l \leq \#C_i$ và $0 \leq m < \#C_i$,
	%
		\item $\geq\!l\,r^-.C_i$ và $\leq\!m\,r^-.C_i$, nếu $\{\mQ, \mI\} \subseteq \Phi^\dag$, $r \in \SigmaDagOR$, $1 \leq i \leq n$, $0 < l \leq \#C_i$ và $0 \leq m < \#C_i$.
	\end{itemize}
	\vspace{-2.0ex}
}
\caption{Các bộ chọn được sử dụng thêm trong thực tế\label{fig:OtherSelectors}}
\end{figure}

\subsection{Gia lượng thông tin trong việc phân hoạch miền}
\label{sec:Chap3.InfoGain}
Cho diễn dịch $\mI$ là một hệ thống thông tin, $X$ và $Y$ là các tập con của $\Delta^\mI$, trong đó $X$ đóng vai trò là tập các mẫu dương của khái niệm cần học, $Y$ đóng vai trò là một khối của phân hoạch.
%
\begin{Definition}
	{\em Entropy} của tập $Y$ đối với tập~$X$ trong miền $\Delta^\mI$ của diễn dịch~$\mI$, ký hiệu là $E_{\Delta^\mI}(Y/X)$, được xác định như sau:
	\begin{equation}
		E_{\Delta^\mI}(Y/X)=
		\begin{cases}
			0, \text{ nếu } Y \cap X = \emptyset \text{ hoặc } Y \subseteq X\\
			\displaystyle - \frac{\sharp XY}{\sharp Y}\log_2\frac{\sharp XY}{\sharp Y}-\frac{\sharp \overline{X}Y}{\sharp Y}\log_2\frac{\sharp \overline{X}Y}{\sharp Y}, \textnormal{nếu ngược lại,} \label{eq:Entropy}
		\end{cases}
	\end{equation}
	trong đó $XY$ đại diện cho tập $X \cap Y$ và $\overline{X}Y$ đại diện cho tập $\overline{X} \cap Y$.\myend
\end{Definition}

\begin{Remark}
	Theo phương trình~\eqref{eq:Entropy}, chúng ta thấy rằng $E_{\Delta^\mI}(Y/X) = 0$ khi và chỉ khi tập $Y$ không bị phân chia bởi tập $X$.\myend
\end{Remark}

\begin{Definition}
	{\em Gia lượng thông tin} của bộ chọn $D$ trong việc chia tập $Y$ đối với tập~$X$ trong $\Delta^\mI$ của diễn dịch $\mI$, ký hiệu là $IG_{\Delta^\mI}(Y/X, D)$, được xác định như sau:
	\begin{equation}
		\!\!IG_{\Delta^\mI}(Y/X, D)\!=\!E_{\Delta^\mI}(Y/X)\!-\!
		\left(\!\!\frac{\sharp D^\mI Y}{\sharp Y}E_{\Delta^\mI}(D^\mI Y/X)\!+\!\frac{\sharp \overline{D^\mI}Y}{\sharp Y}E_{\Delta^\mI}(\overline{D^\mI}Y/X)\!\!\right)\!\! \label{eq:InfoGain}
	\end{equation}
	trong đó $D^\mI Y$ đại diện cho tập $D^\mI \cap Y$ và $\overline{D^\mI}Y$ đại diện cho tập $\overline{D^\mI} \cap Y$.\myend
\end{Definition}

Trong ngữ cảnh $\Delta^\mI$ và $X$ đã rõ ràng, chúng ta viết $E(Y)$ thay cho $E_{\Delta^\mI}(Y/X)$ và $IG(Y, D)$ thay cho $IG_{\Delta^\mI}(Y/X, D)$.

\subsection{Phân hoạch miền của diễn dịch}

Ta nói rằng tập $Y \subseteq \Delta^\mI$ bị {\em phân chia} bởi $E$ nếu tồn tại $a \in E^+$ và $b \in E^-$ sao cho $\{a^\mI, b^\mI\} \subseteq Y$. Một phân hoạch $\mbY = \{Y_1, Y_2, \ldots, Y_n\}$ của $\Delta^\mI$ được gọi là {\em nhất quán} với~$E$ nếu với mọi $1 \leq i \leq n$, $Y_i$ không bị phân chia bởi $E$.

Cho diễn dịch $\mI$ là một hệ thống thông tin huấn luyện trong $\mLSP$. Gọi $A_d \in \SigmaC$ là một khái niệm đại diện cho ``thuộc tính quyết định'', $E = \tuple{E^+, E^-}$ với $E^+ = \{a \mid a^\mI \in A_d^\mI\}$ và $E^- = \{a \mid a^\mI \in (\neg A_d^\mI)\}$ tương ứng là tập các mẫu dương và mẫu âm của~$A_d$ trong~$\mI$. Giả sử rằng $A_d$ có thể được biểu diễn bởi một khái niệm $C$ trong ngôn ngữ con $\mLSPD$, trong đó $\SigmaDag \subseteq \Sigma \setminus \{A_d\}$ và $\PhiDag \subseteq \Phi$. Vấn đề đặt ra là phân hoạch miền $\Delta^\mI$ của~$\mI$ sử dụng các khái niệm của $\mLSPD$ để thu được phân hoạch $\mbY$ nhất quán với $E$.

Ta thấy rằng, nếu $A_d$ xác định được trong $\mLSPD$ bởi một khái niệm $C$, lúc đó:
\begin{itemize}
	\item theo khẳng định thứ nhất của Định lý~\ref{th:Consistent}, $C^\mI$ phải là hợp của một số lớp tương đương trong phân hoạch $\mbY$ của $\Delta^\mI$ được phân hoạch thông qua $\simSdPdI$,
	
	\item $a^\mI \in C^\mI$ với mọi $a \in E^+$ và $a^\mI \notin C^\mI$ với mọi $a \in E^-$, nghĩa là phân hoạch $\mbY$ nhất quán với $E$.
\end{itemize}

\begin{algorithm}[t]
	\fontsize{13.5}{15}\selectfont
	\KwIn{$\mI$, $\SigmaDag$, $\PhiDag$, $E = \tuple{E^-, E^+}$}
	\KwOut{$\mbY = \{Y_{i_1}, Y_{i_2}, \ldots, Y_{i_k}\}$ sao cho $\mbY$ nhất quán với $E$}
	\SetKwFunction{Partition}{Partition}
	\BlankLine
	$n:=1$; $Y_1 := \Delta^\mI$; $\mbY := \{Y_1\}$; $C_1 := \top$; $\mbD := \emptyset$\;
	Tạo và thêm các bộ chọn vào $\mbD$\label{step:CreateSelector1}
	\tcc*[r]{\small theo Định nghĩa~\ref{def:BasicSelectors} và Hình~\ref{fig:OtherSelectors}}
	\While {$(\mbY$ không nhất quán với $E)$ \textnormal{and} $(\mbY$ có thể phân hoạch$)$}
	{
		Chọn $D_u \in \mbD$ và $Y_{i_j} \in \mbY$ sao cho $D_u$ chia $Y_{i_j}$ thành hai khối không rỗng\label{step:ChooseBlock}\;
		$s:=n+1$; $t:=n+2$; $n:=n+2$\;
		$Y_s := Y_{i_j} \cap D_u^\mI$;\qquad\quad\!$C_s := C_{i_j} \mand D_u$\;
		$Y_t := Y_{i_j} \cap (\neg D_u)^\mI$;\quad $C_t := C_{i_j} \mand \neg D_u$\;
		$\mbY := \mbY \cup \{Y_s, Y_t\} \setminus \{Y_{i_j}\}$\;    
		Tạo và thêm các bộ chọn mới vào $\mbD$\label{step:CreateSelector2}\tcc*[r]{\small theo Định nghĩa~\ref{def:BasicSelectors} và Hình~\ref{fig:OtherSelectors}}
	}
	\eIf {$(\mbY$ nhất quán với $E)$}
	{
		\Return $\mbY$\;
	}
	{
		\Return failure;
	}
	\caption{{\em Phân hoạch miền của diễn dịch trong logic mô tả}} \label{alg:Partition}
\end{algorithm}

Với các nhận xét trên, chúng tôi thiết kế Thuật toán~\ref{alg:Partition} để phân hoạch miền của một diễn dịch trong logic mô tả.
%
Giả sử chúng ta có phân hoạch hiện thời là $\mbY= \{Y_{i_1}, Y_{i_2}, \ldots, Y_{i_k}\}$ và tập các bộ chọn hiện thời là $\mbD=\{D_1, D_2, \ldots, D_h\}$ (được xây dựng bằng cách sử dụng các luật trong Định nghĩa~\ref{def:BasicSelectors} và Hình~\ref{fig:OtherSelectors}).
%
Với mỗi khối $Y_{i_j} \in \mbY$, gọi $S_{i_j}$ là bộ chọn đơn giản nhất có được từ $\underset{D_u \in \mbD}{\arg \max}\{IG(Y_{i_j}, D_u)\}$. Bộ chọn $S_{i_j}$ được chọn để phân hoạch khối~$Y_{i_j}$.
Tiếp theo, chúng ta chọn khối \mbox{$Y_{i_j} \in \underset{Y_{i_j} \in \mbY}{\arg \max}{\{IG(Y_{i_j}, S_{i_j})\}}$} để phân chia trước. Quá trình lựa chọn này dựa trên độ đo gia lượng thông tin.
%
\begin{Example} \label{ex:Partition1}
Xét cơ sở tri thức $\KB$ như đã cho trong Ví dụ~\ref{ex:KnowledgeBase3} và diễn dịch $\mI$ là mô hình của $\KB$ như sau:
\begin{eqnarray*}
\Delta^\mI \!\!\!&=&\!\!\! \{\Pub_1, \Pub_2, \Pub_3, \Pub_4, \Pub_5, \Pub_6\},\qquad\;\;\,
x^\mI = x \textrm{ với } x \in \{\Pub_1, \Pub_2, \Pub_3, \Pub_4, \Pub_5, \Pub_6\}, \\
\Publication^\mI \!\!\!&=&\!\!\! \Delta^\mI,\quad\Awarded^\mI = \{\Pub_1, \Pub_4, \Pub_6\}, \quad\UsefulPub^\mI = \{\Pub_2,\Pub_3,\Pub_4,\Pub_5,\Pub_6\}, \\
\Cites^\mI \!\!\!&=&\!\!\! \{\tuple{\Pub_1, \Pub_2}, \tuple{\Pub_1, \Pub_3}, \tuple{\Pub_1, \Pub_4}, \tuple{\Pub_1, \Pub_6}, \tuple{\Pub_2, \Pub_3}, \tuple{\Pub_2, \Pub_4},\\
           \!\!\!& &\!\!\! \;\;\tuple{\Pub_2, \Pub_5}, \tuple{\Pub_3, \Pub_4}, \tuple{\Pub_3, \Pub_5}, 
\tuple{\Pub_3, \Pub_6}, \tuple{\Pub_4, \Pub_5}, \tuple{\Pub_4, \Pub_6}\,\}, \\
\Citedby^\mI \!\!\!&=&\!\!\! (\Cites^\mI)^{-1},\;\; 
\textrm{hàm từng phần $\PubYear^\mI$ được đặc tả theo từng cá thể.}
\end{eqnarray*}

Cho $E = \tuple{E^+, E^-}$ với $E^+ = \{\Pub_4, \Pub_6\}$ và $E^- = \{\Pub_1, \Pub_2, \Pub_3, \Pub_5\}$, ngôn ngữ con $\mLSPD$, trong đó $\SigmaDag = \{\Awarded, \Citedby\}$ và  $\Phi^\dag = \emptyset$. Các bước làm mịn phân hoạch~$\{\Delta^\mI\}$ của diễn dịch $\mI$ được mô tả như sau:
\begin{enumerate}
	\item $Y_1 := \Delta^\mI$, $C_1 := \top$, $\mbY := \{Y_1\}$
	\item Theo độ đo gia lượng thông tin, bộ chọn tốt nhất để phân chia $Y_1$ là $\Awarded$. Phân chia khối $Y_1$ bởi $\Awarded$ chúng ta thu được:
	\begin{itemize}
		\item $Y_2 := \{\Pub_1, \Pub_4, \Pub_6\}$, $C_2 := \Awarded$
		\item $Y_3 := \{\Pub_2, \Pub_3, \Pub_5\}$, $C_3 := \neg\Awarded$
		\item $\mbY := \{Y_2, Y_3\}$
	\end{itemize}
	\item Theo độ đo gia lượng thông tin, các bộ chọn tốt nhất để phân chia khối $Y_2$ là $\E\Citedby.\top$, $\E\Citedby.C_2$ và $\E\Citedby.C_3$. Chúng ta sử dụng bộ chọn đơn giản nhất $\E\Citedby.\top$ để phân chia $Y_2$ và thu được:
	\begin{itemize}
		\item $Y_4 := \{\Pub_4, \Pub_6\}$, $C_4 := C_2 \mand \E\Citedby.\top$
		\item $Y_5 := \{\Pub_1\}$, $C_5 := C_2 \mand \neg\E\Citedby.\top$
		\item $\mbY := \{Y_3, Y_4, Y_5\}$
	\end{itemize}
\end{enumerate}

Phân hoạch đạt được là $\mbY = \{Y_3, Y_4, Y_5\}$ nhất quán với $E$, gồm khối $Y_4$ chứa $\Pub_4$, $\Pub_6$ với $\Pub_4, \Pub_6 \in E^+$ và các khối $Y_3$, $Y_5$ không chứa cá thể nào của $E^+$ nên ta có kết quả trả về là \mbox{$\mbY = \{Y_3, Y_4, Y_5\}$}
(phân hoạch này không tương ứng với quan hệ $\sim_\SdPdI$).\myend
\end{Example}

Quá trình làm mịn phân hoạch của Ví dụ~\ref{ex:Partition1} được minh họa thông qua cây quyết định như trong Hình~\ref{fig:DecisionTree1}.

\begin{figure}[h!]
\ramka{
	\vspace{-3.0ex}
	\begin{center}
	\begin{tabular}{c}
		\xymatrix@C=14ex@R=7ex{
		& *+[F]{\{\Pub_1,\Pub_2,\Pub_3,\Pub_4,\Pub_5,\Pub_6\}}
		\ar@{->}_{\Awarded}[d]
		\ar@{->}^{\;\;\neg\Awarded}[dr]
		\\
		& *+[F]{\{\Pub_1,\Pub_4,\Pub_6\}}
		\ar@{->}_{\neg\E\Citedby.\top\;\;}[dl]
		\ar@{->}^{\E\Citedby.\top}[d]
		&
		*+[F]{\{\Pub_2,\Pub_3,\Pub_5\}}
		\\
		*+[F]{\{\Pub_1\}}
		& *+[F]{\{\Pub_4,\Pub_6\}}
		} % \xymatrix
	\end{tabular}
	\end{center}
	\vspace{-2.0ex}
}
\caption{Cây quyết định minh họa quá trình làm mịn phân hoạch của Ví dụ~\ref{ex:Partition1}\label{fig:DecisionTree1}}
\end{figure}

\section{Học khái niệm trong logic mô tả}
\label{sec:Chap3.ConceptLearning}
\subsection{Thuật toán \BBCLearnS}
\label{sec:Chap3.BBCL2}

\begin{algorithm}[t!]
	\fontsize{13.5}{15}\selectfont
	\KwIn{$\KB$, $\SigmaDag$, $\PhiDag$, $E = \tuple{E^+, E^-}$, $K$}
	\KwOut{Khái niệm $C$ sao cho:
		\begin{itemize}
			\item $\KB \models C(a)$ với mọi $a \in E^+$, và
			\vspace{-0.1cm}
			\item $\KB \not\models C(a)$ for all $a \in E^-$.
		\end{itemize}
	}
	\SetKwFunction{PartitionDomain}{Partition}
	\SetKwFunction{Simplify}{Simplify}
	$E_0^- := \emptyset$; $\mbC:=\emptyset$; $\mbC_0:=\emptyset$\;
	\While {\textnormal{not }$($too hard to extend $\mbC)$ \textnormal{and} $(E_0^- \neq E^-)$}
	{ \label{step:BBCL2While1}
		Xây dựng mô hình hữu hạn $\mI$ của $\KB$ hoặc $\mI = \mI_{\mid K}'$\label{step:BBCL2ConstructModel}\;
		$\mbY := $\PartitionDomain($\mI$, $\SigmaDag$, $\PhiDag$, $E$)\label{step:BBCL2Partition}\tcc*[r]{\small phân hoạch miền của $\mI$}
		\ForEach{$Y_{i_j} \in \mbY$, $\E a \in E^-: a^\mI \in Y_{i_j}$ \textnormal{and} $\V a \in E^+: a^\mI \not\in Y_{i_j}$} 
		{ \label{step:BBCL2ForEach1}
			\eIf {$(\KB \models \neg C_{i_j}(a), \V a \in E^+)$}
			{
				\If {$(\KB \not\models (\bigsqcap\mbC \sqsubseteq \neg C_{i_j}))$}
				{
					$\mbC := \mbC \cup \{\neg C_{i_j}\}$\label{step:BBCL2Add1}\;
					$E_0^- := E_0^- \cup \{a \in E^- \mid a^\mI \in Y_{i_j}\}$\;
				}
			}
			{
				$\mbC_0 := \mbC_0 \cup \{\neg C_{i_j}\}$\;
			}
		}
	}
	\While {\textnormal{not }$($too hard to extend $\mbC)$ \textnormal{and} $(E_0^- \neq E^-)$}
	{ \label{step:BBCL2While2}
		$D:=D_1 \mor D_2 \mor \cdots \mor D_l$, với $D_1, D_2, \ldots, D_l$ được chọn ngẫu nhiên từ $\mbC_0$\;
		\If {$(\KB \models D(a), \V a \in E^+)$}
		{
			\If {$(\KB \not\models (\bigsqcap\mbC) \sqsubseteq D)$ \textnormal{and} $(\E a \in E^-\setminus E_0^-: \KB \not\models (\bigsqcap\mbC)(a))$}
			{
				$\mbC := \mbC \cup \{D\}$\label{step:BBCL2Add2}\;
				$E_0^- := E_0^- \cup \{a \mid a\in E^- \setminus E_0^-, \KB \not\models (\bigsqcap\mbC)(a)\}$\;
			}
		}
	}
	\eIf{$(E_0^- = E^-)$}
	{
		\ForEach{$D \in \mbC$}
		{
			\If{$\KB \not \models \bigsqcap(\mbC \setminus \{D\})(a), \V a \in E^-$}
			{
				$\mbC := \mbC \setminus \{D\}$\label{step:BBCL2Remove}\;
			}
		}
		$C := \bigsqcap\mbC$\label{step:BBCL2Result}\;
		\Return $C_{rs} :=$ \Simplify($C$)\label{step:BBCL2Simplify}\tcc*[r]{\small rút gọn khái niệm $C$}
	}
	{
		\Return failure\;
	}
	\caption{{\em BBCL2--Học khái niệm đối với cơ sở tri thức trong logic mô tả}\label{alg:BBCL2}}
\end{algorithm}

Ý tưởng chính của thuật toán \BBCLearnS để giải quyết bài toán này là sử dụng các mô hình của $\KB$ kết hợp với mô phỏng hai chiều trong mô hình đó (để mô hình hóa tính không phân biệt được) và cây quyết định (để phân lớp dữ liệu) cho việc tìm kiếm khái niệm $C$. Thuật toán này sử dụng Thuật toán~\ref{alg:Partition} để làm mịn phân hoạch~$\{\Delta^\mI\}$ của diễn dịch $\mI$ (là mô hình của $\KB$) nhằm đạt được phân hoạch nhất quán với $E = \tuple{E^+, E^-}$.

Thuật toán tiến hành xây dựng tập $E^-_0$ và mở rộng nó sao cho $E^-_0$ phủ càng lúc càng nhiều cá thể trong $E^-$, xây tập $\mbC$ gồm các phần tử là các khái niệm $D$ sao cho $\KB \models D(a)$ với mọi $a \in E^+$ và xây dựng tập $\mbC_0$ gồm khái niệm để trợ giúp cho việc xây dựng khái niệm $C$. 
Khi một khái niệm $D$ không thỏa mãn điều kiện $\KB \models \neg D(a)$ với mọi $a \in E^-$ nhưng nó là một khái niệm ứng viên ``tốt'' thì khái niệm $D$ được đưa vào $\mbC_0$. Sau này, khi cần thiết, các khái niệm trong $\mbC_0$ được lấy ra, thực hiện phép hợp và kiểm tra xem nó có thỏa mãn điều kiện để thêm vào $\mbC$ hay không. Như vậy, trong quá trình học chúng ta luôn có:
\begin{itemize}
	\item $\KB \models (\bigsqcap\mbC)(a)$ với mọi $a \in E^+$, và
	\item $\KB \not\models (\bigsqcap\mbC)(a)$ với mọi $a \in E^-_0$. 
\end{itemize}

Thuật toán mở rộng $\mbC$ sao cho $\KB \not\models(\bigsqcap\mbC)(a)$ với càng lúc càng nhiều $a \in E^-$. Như vậy, mở rộng $\mbC$ đồng nghĩa với việc mở rộng $E^-_0$. Khi $E^-_0 = E^-$ thuật toán trả về khái niệm $\bigsqcap\mbC$ sau khi đã thực hiện việc chuẩn hóa và đơn giản hóa.

\subsection{Tính đúng của thuật toán BBCL2}

\begin{Proposition}[Tính đúng đắn của thuật toán \BBCLearnS]
	Thuật toán \BBCLearnS là đúng đắn. Nghĩa là, nếu thuật toán \BBCLearnS trả về một khái niệm $C_{rs}$ thì $C_{rs}$ là một lời giải của bài toán học khái niệm cho cơ sở tri thức trong logic mô tả.\myend
\end{Proposition}

\begin{Remark}[Độ phức tạp của thuật toán \BBCLearnS]
Học khái niệm cho cơ sở tri thức trong logic mô tả liên quan chặt chẽ với vấn đề suy luận tự động trong logic mô tả. Đối với vấn đề suy luận tự động, độ phức tạp của bài toán này là \EXPTIME-khó ngay cả đối với logic mô tả cơ bản \ALC. Đến nay, các công trình nghiên cứu vẫn chưa chỉ ra được dấu hiệu nào cho thấy có thể hy vọng làm giảm độ phức tạp của bài toán suy luận. Một cách tổng quát, bài toán 
kiểm tra tính thỏa trong logic mô tả thường là \EXPTIME-đầy đủ.
Thuật toán BBCL2 sử dụng một vòng lặp tuyến tính có giới hạn là lực lượng của $C_0$ cho bài toán suy luận. Do đó thuật toán này có độ phức tạp là hàm mũ (xét theo kích thước của $\KB$, $E^+$ và $E^-$ với giả thiết là $\SigmaDag$ cố định).\myend
\end{Remark}

\subsection{Ví dụ minh họa}
\begin{Example}
\label{ex:Chap3.ConceptLearning1}
	Xét cơ sở tri thức $\KB_0 = \tuple{\mR,\mT,\mA_0}$ như đã cho trong Ví dụ~\ref{ex:KnowledgeBase3} và $E = \tuple{E^+, E^-}$ đối với $E^+ = \{\Pub_4$, $\Pub_6\}$, $E^- = \{\Pub_1$, $\Pub_2$, $\Pub_3$, $\Pub_5\}$, $\SigmaDag = \{\Awarded$, $\Citedby\}$ và $\Phi^\dag = \emptyset$. Học định nghĩa cho khái niệm $A_d$ với $\KB = \tuple{\mR,\mT,\mA}$, trong đó $\mA = \mA_0 \cup \{A_d(a) \mid a \in E^+\} \cup \{\neg A_d(a) \mid a \in E^-\}$. 
	%
	Thuật toán \BBCLearnS thực hiện các bước như sau: 
	\begin{enumerate}
		\item $\mbC := \emptyset$, $\mbC_0 := \emptyset$, $E^-_0: = \emptyset$.
		\item $\KB$ có nhiều mô hình, trong đó mô hình $\mI$ được đặc tả trong Ví dụ~\ref{ex:Partition1} như sau:
		\[
		\!\!\!\!\!\!\!\!\!\begin{array}{r c l}
		\Delta^\mI &=& \{\Pub_1, \Pub_2, \Pub_3, \Pub_4, \Pub_5, \Pub_6\},\qquad x^\mI = x \textrm{ với } x \in \{\Pub_1, \Pub_2, \Pub_3, \Pub_4, \Pub_5, \Pub_6\}, \\[0.7ex]
		\Publication^\mI &=& \Delta^\mI,\;\;\Awarded^\mI = \{\Pub_1, \Pub_4, \Pub_6\}, \;\;\UsefulPub^\mI = \{\Pub_2,\Pub_3,\Pub_4,\Pub_5,\Pub_6\}, \\[0.7ex]
		\Cites^\mI &=& \{\tuple{\Pub_1, \Pub_2}, \tuple{\Pub_1, \Pub_3}, \tuple{\Pub_1, \Pub_4}, \tuple{\Pub_1, \Pub_6}, \tuple{\Pub_2, \Pub_3}, \tuple{\Pub_2, \Pub_4},\\[0.7ex]
		& & \;\;\tuple{\Pub_2, \Pub_5}, \tuple{\Pub_3, \Pub_4}, \tuple{\Pub_3, \Pub_5}, 
		\tuple{\Pub_3, \Pub_6}, \tuple{\Pub_4, \Pub_5}, \tuple{\Pub_4, \Pub_6}\,\}, \\[0.7ex]
		\Citedby^\mI &=& (\Cites^\mI)^{-1},\;\;\textrm{hàm từng phần $\PubYear^\mI$ được đặc tả theo từng cá thể.}
		\end{array}
		\]
		
		\item Áp dụng Thuật toán~\ref{alg:Partition} để làm mịn phân hoạch $\{\Delta^\mI\}$ của $\mI$, chúng ta thu được phân hoạch $\mbY = \{Y_3, Y_4, Y_5\}$ nhất quán với $E$ tương ứng với các khái niệm đặc trưng $C_3, C_4, C_5$, trong đó $Y_3=\{\Pub_2, \Pub_3, \Pub_5\}$, $Y_4 = \{\Pub_4, \Pub_6\}$, $Y_5 = \{\Pub_1\}$ và $C_3 \equiv \neg\Awarded$, $C_4 \equiv \Awarded \mand \E\Citedby.\top$, $C_5 \equiv \neg\Awarded \mand \E\Citedby.\top$. (Xem quá trình phân hoạch ở Ví dụ~\ref{ex:Partition1})

		\item Vì $Y_3 \subseteq E^-$ nên ta tiến hành xem xét đối với $C_3 \equiv \neg\Awarded$. Vì $\KB \models \neg C_3(a)$ với mọi $a \in E^+$ nên ta thêm $\neg C_3$ vào $\mbC$ và thêm các phần tử của $Y_3$ vào $E^-_0$. Do đó, ta có $\mbC = \{C_3\}$ và $E^-_0 = \{\Pub_2,\Pub_3,\Pub_5\}$.
		
		\item Vì $Y_5 \subseteq E^-$ nên ta tiến hành xem xét đối với $C_5 \equiv \Awarded \mand \neg\E\Citedby.\top$. Vì $\KB \models \neg C_5(a)$ với mọi $a \in E^+$ và $\bigsqcap\mbC$ không bị bao hàm bởi $C_5$ dựa trên $\KB$ nên ta thêm $\neg C_5$ vào $\mbC$. Do đó, ta có $\mbC = \{\neg C_3, \neg C_5\}$, $\bigsqcap\mbC \equiv \neg C_3 \mand \neg C_5 \equiv \neg\neg\Awarded \mand \neg (\Awarded \mand \neg\E\Citedby.\top)$ và $E^-_0 = \{\Pub_1,\Pub_2,\Pub_3,\Pub_5\}$.
		
		\item Vì $E^-_0 = E^-$ nên ta có $C \equiv \bigsqcap\mbC \equiv \neg\neg\Awarded \mand \neg(\Awarded \mand \neg \E\Citedby.\top)$. Rút gọn $C$ ta được kết quả trả về là $C_{rs} \equiv \Awarded \mand \E\Citedby.\top$.\myend  
	\end{enumerate}
\end{Example}

%-------------------------------------------------------------------
\section*{Tiểu kết Chương~\ref{Chapter3}}
\addcontentsline{toc}{section}{Tiểu kết Chương~\ref{Chapter3}}
\label{sec:Chap3.Summary}
Chương này đã trình bày thuật toán làm mịn phân hoạch miền của diễn dịch trong logic mô tả (Thuật toán~\ref{alg:Partition}). Trên cơ sở đó, chúng tôi giới thiệu Thuật toán \BBCLearnS để giải quyết bài toán học khái niệm cho cơ sở tri thức trong logic mô tả. 
Ý tưởng chính của thuật toán này là sử dụng các mô hình của cơ sở tri thức kết hợp với mô phỏng hai chiều trong các mô hình đó (để mô hình hóa tính không phân biệt được) và cây quyết định (để phân lớp dữ liệu) cho việc tìm kiếm khái niệm kết quả. Tính đúng của thuật toán \BBCLearnS cũng được chứng minh thông qua bổ đề liên quan.
Thuật toán này có thể áp dụng cho một lớp lớn các logic mô tả là mở rộng của $\mathcal{ALC}_{\Sigma,\Phi}$ có tính chất mô hình hữu hạn hoặc nữa hữu hạn, trong đó $\Phi \subseteq \{\mI, \mF, \mN, \mQ, \mO, \mU, \Self\}$. Lớp các logic mày chứa nhiều logic mô tả rất hữu ích, chẳng hạn như \SHIQ (logic làm cơ sở cho OWL) và \SROIQ (logic làm cơ sở cho OWL~2) được sử dụng trong nhiều ứng dụng của Web ngữ nghĩa.
%\cleardoublepage
